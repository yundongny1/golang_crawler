This is the repository for crawling and scraping wikipedia pages in Go. Using the Colly package, it is relatively simple to write a script to scrape the web.

To start this we create a struct to hold the information we retrieve from webpage. This program only brings in the site URL and the entire body text, but depending on one's needs, one can even scrape the imgURL or any other specific information using HTML and CSS. Because we are using Colly package, we start by creating a collector within the Colly package. Using this collector, we can call on other functions such as onHTML, which is the actual function that we can specify the CSS element we want to scrape from the websites we are visitng. We append whatever result we get into a slice, and using a for loop, visit the list of webpages we defined in our urls slice.

The run time for this program is 1.1218s, compared to the Python Scraper which takes 10.7523 seconds to execute. One thing to note is that our code is simpler than the scraper written in Python because it does not output the html files that the Python program does, but because most of the compile time comes from the crawling and scraping, we can see that Golang is superior to Python in terms of scraping.

To run this program, run 'go run main.go' from the terminal. The output will be output.jsonl file that contains the url and the text for each website you specify in the url slice.
